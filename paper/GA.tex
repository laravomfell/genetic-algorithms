\section{Using Evolutionary Algorithms for Ensemble Selection}
\label{UsingEA}
Ensemble Selection is essentially an optimization problem in a constrained search space. In order to find an effective solution, optimization strategies must exploit all available information. One can use derivative-based information, i.e. on the gradient of the search space. In the case of complex spaces with many local optima, these methods tend to get stuck and miss global solutions. Evolutionary Algorithms capitalize on a different strategy that often makes a suitable trade-off between exploration and exploitation of the search space. Hence, they are a search heuristic worthwhile exploring for ensemble selection.

\subsection{Introduction to Evolutionary Algorithms}
\label{introEA}
Evolutionary Algorithms are nature-inspired optimization algorithms. The idea of emulating natural selection and evolution emerged very early into the development of computational optimization; already in the 1950s, first evolution systems were employed to optimize real-valued functions \cite[p. 2]{mitchell1999introduction}. The first genetic algorithm that mimics the way genes on a chromosome are encoded, was developed by John Holland in the 1960s. In his book \emph{Adaptation in natural and artificial systems}, Holland presented an advanced, more abstract version of the GA that is able to solve many different problem along with a theoretical framework of how GAs optimize \citep{holland1975adaptation}. Genetic Algorithms still follow the structure presented by \citeauthor{holland1975adaptation}. As many terms are derived from biology and might deserve explanation, some concepts and vocabulary are briefly here. EAs are \emph{population}-based, where the population is a group of candidate solutions to the problem at hand. The population can also be called swarm and its size is denoted by $N$. The candidate solution is usually called \emph{chromosome}, but also particle, individual or string. It can be a binary-encoded or a real-valued numeric vector. The candidate solution consists of positions of \emph{genes}, \emph{loci} and contains as many loci as the problem has dimensions. The value at each locus is called \emph{allele}. As an example, in the case of ensemble selection, the problem dimension is equal to the number of models $D$ in the model library. So each chromosome $x$ consists of $D$ genes and the value at $x_i$ corresponds to the weight assigned to model $i \in 1,\dots, D$.

Typically, EAs consist of five stages which are explained here with the example of the canonical Genetic Algorithm: 1. Initialization, where the initial population of dimensions $N \times D$ is randomly created. 2. Evaluation: its fitness is evaluated on some cost function to be optimized. 3. Selection: a fixed number of best performing chromosomes is selected for the next stages. 4. Crossover: two chromosomes are randomly selected and combined to produce offspring. 5. Mutation: the offspring is randomly mutated and included in the population. The steps 2--5 are repeated until a stopping criterion is reached, e.g. the optimum, a fixed number of generations or a convergence criterion.

This basic setup can be varied and extended in numerous ways. Instead of random selection, one can implement fitness-proportionate selection or roulette-wheel selection, where better performing solutions are assigned a higher probability of selection. An alternative is tournament selection where randomly chosen chromosomes "perform again" each other and the best individuals are selected \citep{miller1995genetic}.
There are many options on how to combine the parents to produce offspring, the simplest one being one-point crossover where both parents are split at a random point $i \in 1,\dots, D$ and the first child inherits the characteristics of the first parent up until $i$ from where on it posses the second parent's properties. The inverse is applied to the second child. Another common modification is elitism where a fixed number of best evaluated chromosomes in each generation are preserved and replace the worst performing chromosomes in step 5 and are directly transferred to the next generation. \\



%Considering 
%invalid solutions
%canonical
%implicit parallelism
% too much exploitation: local hillclimbing; too much exploration: loss of valuable information
% cite De Jong 1993 about  p. 14 % which is better
%Additionally, evolutionary algorithms can either optimize binary encoded solutions as well as real-valued solutions. This is representative of the major advantage of evolutionary algorithms compared to other optimization strategies: its adaptiveness. Most evolutionary algorithms fail to find the global optimum (citation), but they are well versed to solve a range of problems and find sufficiently satisfying solutions.

%"A genetic algorithm maintains a population of potential solutions to the objective function being optimized. The initial group of potential solutions is determined randomly. These potential solutions, called "chromosomes," are allowed to evolve over a number of generations. At every generation, the fitness of each chromosome is calculated. The fitness is a measure of how well the potential solution optimizes the objective function. The subsequent generation is created through a process of selection and recombination. The chromosomes are probabilistically chosen for recombination based upon their fitness; this is a measure of how well the chromosomes achieve the desired goal (e.g. find the minimum in a specified function, etc.). The recombination operator combines the information contained within pairs of selected "parents", and places a mixture of the information from both parents into a member of the subsequent generation. Selection and recombination are the mechanisms through which the population "evolves." Although the chromosomes with high fitness values will have a higher probability of being selected for recombination than those which do not, they are not guaranteed to appear in the next generation. The "children" chromosomes produced by the genetic recombination are not necessarily better than their "parent" chromosomes. Nevertheless, because of the selective pressure applied through a number of generations, the overall trend is towards better chromosomes.
%In order to perform extensive search, genetic diversity must be maintained. When diversity is lost, it is possible for the GA to settle into a sub-optimal state. There are two fundamental mechanisms which the basic GA uses to maintain diversity. The first, mentioned above, is a probabilistic scheme for selecting which chromosomes to recombine. This ensures that information other than that represented in the best chromosomes appears in the subsequent generation. Recombining only good chromosomes will very quickly converge the population without extensive exploration, thereby increasing the possibility of finding only a local optimum. The second mechanism is mutation; mutations are used to help preserve diversity and to escape from local optima. Mutations introduce random changes into the population.
EAs are often used in the following scenarios: the problem is very complex or high-dimensional. Also, because they work heuristically and not analytically, discontinuous or non-differentiable objective functions are not an issue to their performance as opposed to derivative-based algorithms. Also, as EAs can deal with the exploration/exploitation trade-off quite well so that search spaces with many local optima do not necessarily impede their convergence. 
Nevertheless, one needs to bear in mind that EAs do not find the analytical function optimum. With finite populations, they may never reach the global optimum or lose it over time due to mutation. EAs are best applied in situations where a reasonably satisfying solution in a reasonable time frame is sufficient. As De Jong argues: the genetic algorithm "is attempting to maximize cumulative payoff from arbitrary landscapes, deceptive or otherwise. In general, this is achieved by not investing too much effort in finding cleverly hidden peaks (the risk/reward ratio is too high)." \cite[p. 15]{de1993genetic}. This notion can be transferred to all EAs \cite[p. 92]{mitchell1999introduction}.\\ %some of the pioneering work [Holland, 1975]
%In order to control this, their parameters must be carefully tuned. A large enough population size for example can help maintain many diverse solution. \\


%Considering the somewhat random nature of how EAs find and maintain solutions, the question rises of why one would expect EAs to outperform any other heuristic algorithm at all. Even more so, as EAs make no assumption on the search space and exploit only the information contained the obtained fitness values to guide their search. HowevAny effective search is based on the tension between exploration and exploitation. Exploration refers to the moving around in the search space to find new information and potentially find global peaks. Exploitation is the use and propagation of local information \cite[p. 64]{karr1999practical}. 

%Why would we ever expect a GA to outperform RMHC on a landscape like R1? In principle, because of implicit parallelism and crossover. If implicit parallelism works correctly on R1, then each of the schemas competing in the relevant partitions in R1 should have a reasonable probability of receiving some samples at each generation—in particular, the schemas with eight adjacent ones in the defining bits should have a reasonable probability of receiving some samples. This amounts to saying that the sampling in each schema region in R1 has to be reasonably independent of the sampling in other, nonoverlapping schema regions. In our GA this was being prevented by hitchhiking—in the run represented in figure 4.2, the samples in the s3 region were not independent of those in the s2 and s4 regions. In RMHC the successive strings examined produce far from independent samples in each schema region: each string differs from the previous string in only one bit. However, it is the constant, systematic exploration, bit by bit, never losing what has been found, that gives RMHC the edge over our GA. %Mitchell 99




%principies of natural selection and genetic recombination.

%The GAs described in the current literature are highly modified to reconcile the differences between function optimization, in which the effectiveness is measured by the best solution found, and maximizing cumulative returns, in which finding the absolute best solution may not be the only critical issue. %Baluja p.5










\subsection{Optimizing Ensemble Selection}
\label{subop}
When using EAs, two fundamental questions need to be decided upon at the start: the function to be optimized and the encoding. EAs can be binary-encoded, meaning that each gene on the chromosome is either 0 or 1. In the case of Ensemble Selection, a 0 at $x_i$ means that the model $i \in 1,\dots D$ is not included and vice versa. For the evaluation the 1's are divided by their sum, i.e. an exemplary vector $x = {01101}$ of dimensions $(1\times 5)$ is divided by the number of 1's, 3 in this case, such that the weights sum up to 1. As an alternative, the solution vector can be real-valued or numerical. Instead of 0/1, the value at $x_i$ can be any value within the boundary, which in the case of ES is confined to $[0,1]$. As the weights need to sum up to 1, they are divided by their cumulative sum. As an example, a preliminary solution vector $x = \{0.1, 0.5, 0.8, 0.2, 0.05\}$ is divided by its sum 1.65 so that the final solution vector is $x = \{0.06, 0.3, 0.48, 0.12, 0.03\}$. To prevent the EAs from including a large number of arbitrarily small weights, sparsity rules can be enforced where weights smaller than 1\% can be excluded. 
%In order to find the optimal weights for the ensemble selection problem, a number of different evolutionary algorithms was considered, each with particular properties that might prove helpful in solving the optimization problem at hand. In the sections below, each of them are shortly introduced. 
Depending on the problem and the encoding, one potential issue with EAs is the production of invalid solutions, as especially the mutation step can change solutions such that they are outside of the defined constraints. In the case of ensemble weights, these constraints are $0 \leq w_i \leq 1$ and $\sum_{i = 1}^{D} w_i = 1$. 

EAs in general have a tendency to overfit and there is some experimental evidence that they overfit ensemble selection problems (\cite{Robilliard2002}, \cite{radtke2006impact}, \cite{dos2009overfitting}). Complex validation strategies can be implemented to prevent this; their performance however is not overly effective and the effect size of overfitting is rather small \cite[p. 1424]{dos2008overfitting}. In this paper, bagging as described by \cite{caruana2004ensemble} is implemented to test whether a) overfitting is an issue and b) if it can be mitigated by bagging and c) whether bagging improves the solutions.

